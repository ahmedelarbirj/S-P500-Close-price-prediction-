# -*- coding: utf-8 -*-
"""Copy of lstm univariate S&P500.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K26FmTiJwIRovAl5j5xb6jLZFW-Vrfdr
"""

import numpy as np
import pandas as pd
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.recurrent import LSTM
from keras.models import Sequential
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

from datetime import date, timedelta, datetime
!pip install yfinance
import yfinance as yf
# Setting the timeframe for the data extraction
end_date =  '2022-08-21'
start_date = '2007-01-01'

# Getting S&P500 quotes
stockname = 'S&P500'
symbol = '^GSPC'

# You can either use webreader or yfinance to load the data from yahoo finance
# import pandas_datareader as webreader
# df = webreader.DataReader(symbol, start=start_date, end=end_date, data_source="yahoo")

dataset = yf.download(symbol, start=start_date, end=end_date)
# Quick overview of dataset
dataset.head()

sd = pd.DataFrame(dataset.Close)
print(sd.describe())

sd.quantile(.95)

close_price = dataset['Close'].values.reshape(-1,1)
close_price

int(len(close_price) *0.2)

dataset['Close'][-789:]

def create_datasets(dataset, sequence_length):
  sequence_length +=1
  seq_dataset = []
  for i in range(len(dataset)-sequence_length):
    seq_dataset.append(dataset[i: i+ sequence_length])

  seq_dataset = np.array(seq_dataset)
  data_x = seq_dataset[:, :-1]
  data_y = seq_dataset[:, -1]
  return data_x, data_y
scaler= MinMaxScaler(feature_range=(0, 1))
close_price_scaled = scaler.fit_transform(close_price)
train_size = int(len(close_price_scaled) *0.8)
#train_data_len = math.ceil(scaled_data.shape[0] * 0.8)
test_size = len(close_price_scaled)-train_size
train, test = close_price_scaled[0:train_size,:], close_price_scaled[train_size:len(close_price_scaled)]

look_back = 30 # sequence length

x_train, y_train = create_datasets(train,look_back)
x_test, y_test = create_datasets(test,look_back)

x_train.shape

y_test.shape

model = Sequential()

#model.add(LSTM(128,return_sequences=True))
model.add(LSTM(50, input_shape=(x_train.shape[1],x_train.shape[2]),return_sequences=False))
model.add(Dropout(0.35))
model.add(Dense(1))
model.add(Activation('linear'))

model.compile(loss='mse', optimizer = 'adam')

history= model.fit(x_train,y_train, batch_size=64, epochs=100, verbose=2, validation_split=0.2)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

train_predict = model.predict(x_train)
test_predict = model.predict(x_test)

train_predict_unnorm = scaler.inverse_transform(train_predict)
test_predict_unnorm = scaler.inverse_transform(test_predict)
#creating similar dataset to plot training predictions
trainpredictPlot= np.empty_like(close_price)

print(trainpredictPlot.shape)
trainpredictPlot[look_back:len(train_predict_unnorm)+look_back, :] = train_predict_unnorm


#creating similar dataset to plot test prediction
testpredictPlot = np.empty_like(close_price)
testpredictPlot[:, :] = np.nan 
#testpredictPlot[len(train_predict_unnorm)+(look_back*2)+30:len(close_price)-10, :] = test_predict_unnorm #30=1, 10=1
testpredictPlot[len(train_predict_unnorm)+(look_back*2)+1:len(close_price)-1, :] = test_predict_unnorm #30=1, 10=1

plt.figure(figsize=(19,10))
plt.plot(close_price, 'g', label= 'original dataset')
plt.plot(trainpredictPlot, 'r', label= 'training dataset')
plt.plot(testpredictPlot, 'b', label= 'predicted and test')
plt.legend(loc= 'upper left')
plt.title("S&P500")
plt.xlabel('Time in days')
plt.ylabel('price')
plt.show()

y_test1=y_test[26:]
test_predict1=test_predict[26:]

from sklearn.metrics import mean_absolute_percentage_error,mean_squared_error,max_error,r2_score
# Mean Absolute Percentage Error (MAPE)
e1 = mean_absolute_percentage_error(y_test, test_predict)
e2 = mean_squared_error(y_test, test_predict)
e3 = r2_score(y_test, test_predict)
print("MAPE=",e1)
print("MSE=",e2)
print("r2_score=",e3)

e1 = mean_absolute_percentage_error(y_test1, test_predict1)
e2 = mean_squared_error(y_test1, test_predict1)
e3 = r2_score(y_test1, test_predict1)
print("MAPE=",e1)
print("MSE=",e2)
print("r2_score=",e3)

y_test.shape

def MAPE(y_test, test_predict_unnorm): 
    y_true, y_pred = np.array(y_test), np.array(test_predict)
    return np.mean(np.abs((y_test - test_predict) / y_test)) * 100

from sklearn.metrics import mean_squared_error, mean_absolute_error
mape=np.mean(np.abs(np.array(test_predict[:])-np.array(y_test))/np.abs(y_test))
print('MAPE: '+str(mape))

from google.colab import drive 
drive.mount('/content/gdrive')
y= pd.DataFrame(test_predict_unnorm)
y.to_excel(excel_writer=r'/content/gdrive/MyDrive/pythonexcel.xlsx')

len(test_predict_unnorm)

real_values= dataset['Close'][-757:]
y= pd.DataFrame(real_values)
y.to_excel(excel_writer=r'/content/gdrive/MyDrive/arima.xlsx')